# LSTM-text-prediction

This project focuses on generating text using an LSTM-based model trained to predict and extend a given text input. The `generate_seq` function takes a starting text and generates a sequence of words by tokenizing the input, feeding it into the model, and predicting the most probable next words iteratively. The model's sequential structure enables it to capture context and long-term dependencies in text data, producing syntactically and contextually coherent outputs. The implementation incorporates preprocessing with tokenization, handles edge cases like empty sequences, and utilizes eager execution for debugging. This text generation approach has applications in predictive text systems, creative writing tools, and domain-specific content generation.
